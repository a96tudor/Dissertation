\begin{document}
	\chapter{Conclusion}
		In this dissertation, I have outlined the work done in designing, implementing and evaluating a machine-learning based extension for the CADETS user interface with the purpose of filtering graph-structured OS-level provenance data. It classifies the nodes either into nodes that are of interest or nodes that are not of interest, from a cybersecurity perspective. As expected, by analysing the several datasets, I observed that these two classes are highly imbalanced (i.e. there are more nodes that don't represent a potential threat than nodes that do). This imbalance in the data labels and the fact that there are multiple node types that had to be represented add a degree of complexity to the problem at hand. During this dissertation, I have shown how these problems were overcome using suitable pre-processing techniques and machine learning algorithms. 
		\\ \\
		The project requirements were all successfully achieved and a set of extensions were implemented. The evaluation of the machine learning models analyses both their performance on the given data through the metrics involved and their applicability to the problem at hand. 
		
	\section{Lessons learnt} 
		This project gave me the opportunity to explore several machine learning techniques, both parametric and non-parametric, and how they can be applied to real-world data. The Neo4J-stored graph cannot be passed directly to the machine learning models. For this reason, I also explored a few data processing techniques in order to come up with a feature-vector representation for every node. Finally, I explored different network-based architectures that would allow the machine learning models to run as an independent entity, for the CADETS UI to make requests to.
		\\ \\
		Because the machine learning models represent the core module of this project and each model in part underwent continuous fine-tuning, the iterative and incremental development model has proven to be appropriate choice for the project at hand. The choice of Python as the main programming language was also appropriate, having support for data processing and machine learning development through libraries such as Numpy and Keras. Moreover, it also provides support for handling incoming network requests through the Flask library, essential in developing the REST API that wraps around the machine learning models.
		\\ \\
		If I were to do the project again, I would have allowed even more time for exploring different machine learning techniques in order to be able to implement models that use even more of the relational nature of the graph data I am working on. On top of that, I would have allowed more time for fine-tuning the neural networks, given the fact that having a performant machine learning model does not involve a pre-defined set of steps, but it is rather a matter of trial and error.
	\section{Further work}
		Working on this project gave me the opportunity to explore areas of Computer Science that were intriguing, but still very little known to me. Personally, I find that this was an extraordinary learning experience and I consider the project a success. However, there are many possible extensions that can be implemented in order to make it even more successful, including:
		\begin{enumerate}
			\item \textit{Exploring more complex neural networks} - Machine learning applied on graphs is a current research trend, with more and more architectures being designed specifically for graph-structured data. Implementing one of these NN architectures would allow me to make more use of the relational nature of the graph-structured data than I currently do trough my models. Two of the models that I would like to implement are Graph Convolutional Networks \cite{kipf2017semi} and Graph Attention Networks \cite{2017arXiv171010903V}.
			
			\item \textit{Solving the Neo4J access bottleneck} - Overcome the low time performance of the library that facilitates the Neo4J connection. One potential solution for this problem would be to replicate the database over multiple instances and perform the feature extraction in parallel. 
				
			\item \textit{Implementing machine learning models that can adapt to user input } - In the final stage, the machine learning models filtering the graph data should accept and adapt from user input, rather than just using a set of pre-defined ground-truths. Therefore, the logical next step would be to extend the machine learning models such as they can easily do this, while preserving their performance.
	\end{enumerate}
	
\end{document}